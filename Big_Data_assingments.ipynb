{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Ecosystem Assignment\n",
    "\n",
    "## Question 1: Explain the core components of the Hadoop ecosystem and their respective roles in processing and storing big data. Provide a brief overview of HDFS, MapReduce, and YARN.\n",
    "\n",
    "**HDFS (Hadoop Distributed File System):**\n",
    "- **Role:** HDFS is designed for storing large datasets reliably and to stream those datasets at high bandwidth to user applications.\n",
    "- **Key Features:** It is highly fault-tolerant and is designed to be deployed on low-cost hardware. It provides high throughput access to application data and is suitable for applications with large datasets.\n",
    "\n",
    "**MapReduce:**\n",
    "- **Role:** MapReduce is a programming model for processing large datasets in parallel across a Hadoop cluster.\n",
    "- **Key Features:** It breaks down the processing into map and reduce tasks. The map tasks process input data and produce intermediate results, while reduce tasks aggregate these intermediate results to produce the final output.\n",
    "\n",
    "**YARN (Yet Another Resource Negotiator):**\n",
    "- **Role:** YARN manages resources in a Hadoop cluster and schedules jobs. It allows multiple data processing engines such as batch processing, interactive processing, and real-time processing to run and process data stored in HDFS.\n",
    "- **Key Features:** YARN decouples the resource management and job scheduling from the data processing component (MapReduce).\n",
    "\n",
    "## Question 2: Discuss the Hadoop Distributed File System (HDFS) in detail. Explain how it stores and manages data in a distributed environment. Describe the key concepts of HDFS, such as NameNode, DataNode, and blocks, and how they contribute to data reliability and fault tolerance.\n",
    "\n",
    "**HDFS (Hadoop Distributed File System):**\n",
    "- **Storage and Management:** HDFS divides files into large blocks (default size is 128 MB) and distributes them across multiple nodes in a cluster. It replicates each block across multiple DataNodes to ensure fault tolerance.\n",
    "- **NameNode:** The master server that manages the file system namespace and controls access to files by clients. It keeps track of the metadata, like which blocks make up a file and where those blocks are located.\n",
    "- **DataNode:** Nodes where the actual data blocks are stored. DataNodes are responsible for serving read and write requests from the file systemâ€™s clients.\n",
    "- **Blocks:** The basic unit of storage in HDFS. Files are split into blocks and stored across DataNodes. Blocks are replicated to handle hardware failures.\n",
    "\n",
    "**Reliability and Fault Tolerance:**\n",
    "- HDFS ensures data reliability by replicating each block on multiple DataNodes (default replication factor is 3).\n",
    "- If a DataNode fails, the system can read from the replicated blocks stored on other DataNodes.\n",
    "- The NameNode maintains the replication factor by creating additional copies of blocks as needed.\n",
    "\n",
    "## Question 3: Write a step-by-step explanation of how the MapReduce framework works. Use a real-world example to illustrate the Map and Reduce phases. Discuss the advantages and limitations of MapReduce for processing large datasets.\n",
    "\n",
    "**MapReduce Framework:**\n",
    "1. **Input Data Splitting:** The input data is split into fixed-size chunks (typically 128 MB to 256 MB).\n",
    "2. **Map Phase:** Each split is processed by a Map task. The Map function takes input key-value pairs and produces a set of intermediate key-value pairs.\n",
    "   - **Example:** Counting the frequency of words in a document.\n",
    "     - Input: A large text document.\n",
    "     - Map Function: Takes a line of text as input and emits each word with a count of 1.\n",
    "     - Intermediate Output: (\"word1\", 1), (\"word2\", 1), (\"word1\", 1), ...\n",
    "3. **Shuffle and Sort:** The intermediate key-value pairs are shuffled and sorted by key so that all values associated with the same key are grouped together.\n",
    "4. **Reduce Phase:** Each group of intermediate key-value pairs is processed by a Reduce task. The Reduce function takes an intermediate key and a set of values for that key and merges them to form a possibly smaller set of values.\n",
    "   - **Example:** Summing the word counts.\n",
    "     - Reduce Function: Takes (\"word1\", [1, 1, 1, ...]) and sums the counts.\n",
    "     - Output: (\"word1\", total_count), (\"word2\", total_count), ...\n",
    "\n",
    "**Advantages:**\n",
    "- Scalability: Can process petabytes of data.\n",
    "- Fault Tolerance: Automatic handling of node failures.\n",
    "- Flexibility: Can process structured and unstructured data.\n",
    "\n",
    "**Limitations:**\n",
    "- Latency: High latency for real-time data processing.\n",
    "- Complexity: Requires writing custom Map and Reduce functions.\n",
    "- Overhead: Significant overhead in the shuffle and sort phase.\n",
    "\n",
    "## Question 4: Explore the role of YARN in Hadoop. Explain how it manages cluster resources and schedules applications. Compare YARN with the earlier Hadoop 1.x architecture and highlight the benefits of YARN.\n",
    "\n",
    "**YARN (Yet Another Resource Negotiator):**\n",
    "- **Role:** YARN is the resource management layer of Hadoop. It allocates resources to various applications running in a Hadoop cluster.\n",
    "- **Resource Management:** YARN uses ResourceManager and NodeManager for resource management.\n",
    "  - **ResourceManager:** The central authority that arbitrates resources among all the applications in the system.\n",
    "  - **NodeManager:** Runs on each node and is responsible for managing the execution of containers on that node.\n",
    "- **Scheduling:** YARN supports different types of schedulers like FIFO, Capacity Scheduler, and Fair Scheduler to manage how jobs are scheduled.\n",
    "\n",
    "**Comparison with Hadoop 1.x:**\n",
    "- **Hadoop 1.x:** Resource management and job scheduling were handled by JobTracker, and TaskTracker managed the execution of individual tasks on each node. JobTracker was a single point of failure.\n",
    "- **YARN:** Separates resource management and job scheduling into different daemons, ResourceManager and ApplicationMaster, respectively. This separation enhances the scalability and efficiency of the cluster. YARN provides a more flexible and efficient resource management system, supporting multiple data processing engines other than MapReduce.\n",
    "\n",
    "**Benefits of YARN:**\n",
    "- Scalability: Better resource management leads to improved scalability.\n",
    "- Flexibility: Supports different processing models (batch, interactive, streaming).\n",
    "- Resource Utilization: More efficient resource utilization by allowing multiple applications to share the same resources.\n",
    "\n",
    "## Question 5: Provide an overview of some popular components within the Hadoop ecosystem, such as HBase, Hive, Pig, and Spark. Describe the use cases and differences between these components. Choose one component and explain how it can be integrated into a Hadoop ecosystem for specific data processing tasks.\n",
    "\n",
    "**HBase:**\n",
    "- **Overview:** A distributed, scalable, big data store. It is modeled after Google's Bigtable and is used for random, real-time read/write access to large datasets.\n",
    "- **Use Cases:** Real-time querying of large datasets, storing versioned data.\n",
    "\n",
    "**Hive:**\n",
    "- **Overview:** A data warehouse infrastructure built on top of Hadoop. It provides tools to enable easy data summarization, querying, and analysis of large datasets stored in HDFS.\n",
    "- **Use Cases:** SQL-like querying on large datasets, data summarization.\n",
    "\n",
    "**Pig:**\n",
    "- **Overview:** A high-level platform for creating programs that run on Hadoop. The language for this platform is called Pig Latin, and it abstracts the programming model of MapReduce.\n",
    "- **Use Cases:** Complex data transformations, ETL (Extract, Transform, Load) processes.\n",
    "\n",
    "**Spark:**\n",
    "- **Overview:** An open-source unified analytics engine for large-scale data processing. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.\n",
    "- **Use Cases:** Real-time data processing, machine learning, graph processing.\n",
    "\n",
    "**Integration Example: Hive in Hadoop Ecosystem:**\n",
    "- Hive can be integrated into a Hadoop ecosystem for running SQL-like queries on large datasets stored in HDFS. It translates SQL-like queries into MapReduce jobs, making it easier for users who are familiar with SQL to query big data without needing to write MapReduce code directly.\n",
    "\n",
    "## Question 6: Explain the key differences between Apache Spark and Hadoop MapReduce. How does Spark overcome some of the limitations of MapReduce for big data processing tasks?\n",
    "\n",
    "**Differences between Apache Spark and Hadoop MapReduce:**\n",
    "- **Data Processing:** MapReduce processes data in batch mode, while Spark can process data in both batch and real-time modes.\n",
    "- **Data Storage:** MapReduce stores intermediate results in HDFS, which introduces I/O overhead. Spark processes data in-memory, significantly reducing I/O operations and increasing performance.\n",
    "- **Ease of Use:** Spark provides high-level APIs in Java, Scala, and Python, making it easier to use. MapReduce requires users to write complex Java code.\n",
    "- **Performance:** Spark is generally faster than MapReduce due to its in-memory processing capabilities.\n",
    "\n",
    "**How Spark Overcomes Limitations of MapReduce:**\n",
    "- **In-Memory Processing:** By processing data in-memory, Spark reduces the need for frequent disk I/O operations.\n",
    "- **Unified Engine:** Spark offers a unified engine for batch processing, real-time streaming, machine learning, and graph processing.\n",
    "- **Fault Tolerance:** Spark's Resilient Distributed Datasets (RDDs) provide fault tolerance by tracking the lineage of transformations.\n",
    "\n",
    "## Question 7: Write a Spark application in Scala or Python that reads a text file, counts the occurrences of each word, and returns the top 10 most frequent words. Explain the key components and steps involved in this application.\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkContext\n",
    "sc = SparkContext(\"local\", \"Word Count\")\n",
    "\n",
    "# Read the input file\n",
    "text_file = sc.textFile(\"path/to/textfile.txt\")\n",
    "\n",
    "# Perform word count\n",
    "word_counts = (text_file\n",
    "               .flatMap(lambda line: line.split())\n",
    "               .map(lambda word: (word, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7: Write a Spark application in Scala or Python that reads a text file, counts the occurrences of each word,\n",
    "# and returns the top 10 most frequent words. Explain the key components and steps involved in this application.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "\n",
    "# Read text file\n",
    "text_file = spark.read.text(\"path/to/textfile.txt\")\n",
    "\n",
    "# Split each line into words and count occurrences\n",
    "word_counts = text_file.rdd.flatMap(lambda line: line.value.split(\" \")) \\\n",
    "                           .map(lambda word: (word, 1)) \\\n",
    "                           .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Get the top 10 most frequent words\n",
    "top_10_words = word_counts.map(lambda x: (x[1], x[0])) \\\n",
    "                          .sortByKey(False) \\\n",
    "                          .take(10)\n",
    "\n",
    "# Show the result\n",
    "for count, word in top_10_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "# Explanation:\n",
    "# 1. **SparkSession:** The entry point for Spark functionality.\n",
    "# 2. **Reading the text file:** Load the data into an RDD.\n",
    "# 3. **FlatMap:** Split each line into words.\n",
    "# 4. **Map:** Transform words into (word, 1) pairs.\n",
    "# 5. **ReduceByKey:** Aggregate the counts of each word.\n",
    "# 6. **SortByKey:** Sort the words by their counts in descending order.\n",
    "# 7. **Take:** Retrieve the top 10 words with the highest counts.\n",
    "\n",
    "# Question 8: Using Spark RDDs (Resilient Distributed Datasets), perform the following tasks on a dataset of your choice:\n",
    "# a. Filter the data to select only rows that meet specific criteria.\n",
    "# b. Map a transformation to modify a specific column in the dataset.\n",
    "# c. Reduce the dataset to calculate a meaningful aggregation (e.g., sum, average).\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"RDDOperations\").getOrCreate()\n",
    "\n",
    "# Load a dataset\n",
    "data = [(\"Alice\", 29), (\"Bob\", 31), (\"Cathy\", 23), (\"David\", 34)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# a. Filter the data to select only rows where age > 30\n",
    "filtered_rdd = rdd.filter(lambda x: x[1] > 30)\n",
    "print(\"Filtered RDD:\", filtered_rdd.collect())\n",
    "\n",
    "# b. Map a transformation to modify the age column by adding 1\n",
    "mapped_rdd = filtered_rdd.map(lambda x: (x[0], x[1] + 1))\n",
    "print(\"Mapped RDD:\", mapped_rdd.collect())\n",
    "\n",
    "# c. Reduce the dataset to calculate the sum of ages\n",
    "sum_ages = mapped_rdd.map(lambda x: x[1]).reduce(lambda a, b: a + b)\n",
    "print(\"Sum of Ages:\", sum_ages)\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "# Question 9: Create a Spark DataFrame in Python or Scala by loading a dataset (e.g., CSV or JSON) and perform the following operations:\n",
    "# a. Select specific columns from the DataFrame.\n",
    "# b. Filter rows based on certain conditions.\n",
    "# c. Group the data by a particular column and calculate aggregations (e.g., sum, average).\n",
    "# d. Join two DataFrames based on a common key.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
    "\n",
    "# Load a dataset (CSV file)\n",
    "df = spark.read.csv(\"path/to/csvfile.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# a. Select specific columns from the DataFrame\n",
    "selected_df = df.select(\"column1\", \"column2\")\n",
    "print(\"Selected Columns:\")\n",
    "selected_df.show()\n",
    "\n",
    "# b. Filter rows based on certain conditions\n",
    "filtered_df = df.filter(df[\"column1\"] > 30)\n",
    "print(\"Filtered Rows:\")\n",
    "filtered_df.show()\n",
    "\n",
    "# c. Group the data by a particular column and calculate aggregations\n",
    "grouped_df = df.groupBy(\"column1\").agg({\"column2\": \"sum\", \"column3\": \"avg\"})\n",
    "print(\"Grouped Data with Aggregations:\")\n",
    "grouped_df.show()\n",
    "\n",
    "# d. Join two DataFrames based on a common key\n",
    "df1 = spark.read.csv(\"path/to/csvfile1.csv\", header=True, inferSchema=True)\n",
    "df2 = spark.read.csv(\"path/to/csvfile2.csv\", header=True, inferSchema=True)\n",
    "joined_df = df1.join(df2, df1[\"common_key\"] == df2[\"common_key\"])\n",
    "print(\"Joined DataFrames:\")\n",
    "joined_df.show()\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "# Question 10: Set up a Spark Streaming application to process real-time data from a source (e.g., Apache Kafka or a simulated data source).\n",
    "# The application should:\n",
    "# a. Ingest data in micro-batches.\n",
    "# b. Apply a transformation to the streaming data (e.g., filtering, aggregation).\n",
    "# c. Output the processed data to a sink (e.g., write to a file, a database, or display it).\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Initialize a Spark session and Streaming context\n",
    "spark = SparkSession.builder.appName(\"SparkStreamingApp\").getOrCreate()\n",
    "ssc = StreamingContext(spark.sparkContext, 1)  # 1 second batch interval\n",
    "\n",
    "# Simulated data source (socket stream)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# a. Ingest data in micro-batches\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "# b. Apply a transformation (word count)\n",
    "word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# c. Output the processed data to the console\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start the streaming computation\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n",
    "# Question 11: Explain the fundamental concepts of Apache Kafka. What is it, and what problems does it aim to solve in the context of big data and real-time data processing?\n",
    "\n",
    "\"\"\"\n",
    "**Apache Kafka:**\n",
    "- **Concepts:** Kafka is a distributed streaming platform used for building real-time data pipelines and streaming applications. It is designed to handle high throughput and fault tolerance.\n",
    "- **Problems Solved:**\n",
    "  - **Real-Time Data Processing:** Kafka enables real-time processing of data streams, making it possible to build applications that can respond to data as it is generated.\n",
    "  - **Scalability:** Kafka can handle large volumes of data and can scale horizontally by adding more brokers to the cluster.\n",
    "  - **Durability and Fault Tolerance:** Kafka stores data in a durable and fault-tolerant manner, ensuring that data is not lost even in the event of hardware failures.\n",
    "\"\"\"\n",
    "\n",
    "# Question 12: Describe the architecture of Kafka, including its key components such as Producers, Topics, Brokers, Consumers, and ZooKeeper.\n",
    "# How do these components work together in a Kafka cluster to achieve data streaming?\n",
    "\n",
    "\"\"\"\n",
    "**Kafka Architecture:**\n",
    "- **Producers:** Producers are applications that send data (messages) to Kafka topics.\n",
    "- **Topics:** Topics are categories or feed names to which records are published. A topic is split into partitions to enable parallel processing.\n",
    "- **Brokers:** Brokers are Kafka servers that store data and serve client requests. Each broker holds one or more partitions of a topic.\n",
    "- **Consumers:** Consumers are applications that read data from Kafka topics.\n",
    "- **ZooKeeper:** ZooKeeper manages the Kafka cluster metadata and coordinates the brokers, ensuring leader election and configuration management.\n",
    "\n",
    "**Data Streaming Workflow:**\n",
    "1. Producers send data to a specific topic.\n",
    "2. Kafka brokers receive and store the data in partitions.\n",
    "3. Consumers subscribe to topics and consume the data from the partitions.\n",
    "4. ZooKeeper coordinates the brokers, ensuring that the system operates smoothly.\n",
    "\"\"\"\n",
    "\n",
    "# Question 13: Create a step-by-step guide on how to produce data to a Kafka topic using a programming language of your choice and then consume that data from the topic.\n",
    "# Explain the role of Kafka producers and consumers in this process.\n",
    "\n",
    "# Step-by-Step Guide (Python):\n",
    "\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "\n",
    "# Produce data to Kafka topic\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "\n",
    "# Send messages to the topic 'my_topic'\n",
    "producer.send('my_topic', b'Hello, Kafka!')\n",
    "producer.send('my_topic', b'Another message')\n",
    "producer.flush()\n",
    "\n",
    "# Consume data from Kafka topic\n",
    "consumer = KafkaConsumer('my_topic', bootstrap_servers='localhost:9092')\n",
    "\n",
    "# Print consumed messages\n",
    "for message in consumer:\n",
    "    print(f\"Received message: {message.value}\")\n",
    "\n",
    "# Explanation:\n",
    "# - **Producers:** Producers are responsible for sending data to Kafka topics. In this example, we create a producer and send messages to the topic 'my_topic'.\n",
    "# - **Consumers:** Consumers subscribe to topics and read the data. In this example, we create a consumer that reads messages from 'my_topic' and prints them.\n",
    "\n",
    "# Question 14: Discuss the importance of data retention and data partitioning in Kafka. How can these features be configured, and what are the implications for data storage and processing?\n",
    "\n",
    "\"\"\"\n",
    "**Data Retention:**\n",
    "- **Importance:** Data retention determines how long Kafka retains messages in the topic. This is crucial for ensuring that consumers can read data even if they are temporarily offline.\n",
    "- **Configuration:** Retention can be configured using the `log.retention.hours` property for the retention period and `log.retention.bytes` for the maximum size of the log.\n",
    "\n",
    "**Data Partitioning:**\n",
    "- **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
